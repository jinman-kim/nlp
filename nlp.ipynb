{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24265965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp310-cp310-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.9/288.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.3.23 tqdm-4.65.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "596a2328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ac844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# ssl 에러 발생 , nltk_data 경로 식별이 안될때\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de841d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fc742dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "072f8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13736685",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37ff3bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policy',\n",
       " 'doing',\n",
       " 'organization',\n",
       " 'have',\n",
       " 'going',\n",
       " 'love',\n",
       " 'life',\n",
       " 'fly',\n",
       " 'dy',\n",
       " 'watched',\n",
       " 'ha',\n",
       " 'starting']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "851f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lemmatizer.lemmatize('dies',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "791b5b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "842f3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어(stopword) : 의미 분석에 도움안되는 것들\n",
    "# nltk.corpus 에서 제공해줌, 그냥 i, me , my 같은 언어 리스트임\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04c136f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = stopwords.words('english')[:10]\n",
    "example = \"most important is doing ourselves, not you or only me my self\"\n",
    "stop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "812d8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [i for i in word_tokenize(example) if i not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8d6b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 13\n"
     ]
    }
   ],
   "source": [
    "print(result.__len__(), word_tokenize(example).__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38bb58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Korean Text\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eca3c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a22c5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_exam = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "80ceed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_exam.split(' '))\n",
    "word_tokens = word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75eab279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['고기를',\n",
       " '구우려고',\n",
       " '하면',\n",
       " '.',\n",
       " '고기라고',\n",
       " '다',\n",
       " '아니거든',\n",
       " '.',\n",
       " '예컨대',\n",
       " '삼겹살을',\n",
       " '때는',\n",
       " '중요한',\n",
       " '있지',\n",
       " '.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [word for word in word_tokens if word not in stop_words]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b8b4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c7b6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8d95bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(raw_text)\n",
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english')) #영어 불용어\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7ea984f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f221a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장별 전처리\n",
    "# 1) 소문자 혹은 대문자로 처리\n",
    "# 2) stopwords 인지 판단\n",
    "# 3) 길이가 2 이상인 것만 리스트에 넣고, 보캐브러리 딕셔너리에 할당해줌\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2 :\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "    preprocessed_sentences.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae93c703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bc4ecfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집학: {'barber': 17, 'person': 7, 'good': 2, 'huge': 10, 'knew': 2, 'secret': 12, 'kept': 8, 'word': 4, 'keeping': 4, 'driving': 2, 'crazy': 2, 'went': 2, 'mountain': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'단어 집학:', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "015f0d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 17),\n",
       " ('secret', 12),\n",
       " ('huge', 10),\n",
       " ('kept', 8),\n",
       " ('person', 7),\n",
       " ('word', 4),\n",
       " ('keeping', 4),\n",
       " ('good', 2),\n",
       " ('knew', 2),\n",
       " ('driving', 2),\n",
       " ('crazy', 2),\n",
       " ('went', 2),\n",
       " ('mountain', 2)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:-x[1]) #빈도수 정렬 리스트, 엘리먼트는 튜플\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "14894a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 높은 순서대로 단어를 정렬\n",
    "# 빈도 상위 5위만 ,\n",
    "freq_vocab = { word[0]:i for i,word in enumerate(vocab_sorted, start=1) if i < 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4325a49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0eefbdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 7}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OOV : Out-Of-Vocabulary , 단어 집합에 없는 단어, index+1 로 딕셔너시 생성\n",
    "freq_vocab['OOV'] = len(freq_vocab) + 1\n",
    "freq_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "84632506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 6, 5],\n",
       " [1, 3, 5],\n",
       " [6, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [6, 6, 3, 2, 6, 1, 6],\n",
       " [1, 6, 3, 6]]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences = [[freq_vocab[j] if j in freq_vocab else 6 for j in i]for i in preprocessed_sentences]\n",
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fc12c83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter 모듈도 사용가능\n",
    "from collections import Counter\n",
    "vocab = Counter(sum(preprocessed_sentences,[])).most_common(5)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "463431c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d363a042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, ...})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d66a6319",
   "metadata": {},
   "source": [
    "# 패딩 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6e615",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dbc98714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장들 중 max_length 로 패딩 해줘야함\n",
    "max_len  = max(len(i) for i in encoded_sentences)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48428b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f8d79ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5, 0, 0, 0, 0, 0],\n",
       "       [1, 6, 5, 0, 0, 0, 0],\n",
       "       [1, 3, 5, 0, 0, 0, 0],\n",
       "       [6, 2, 0, 0, 0, 0, 0],\n",
       "       [2, 4, 3, 2, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0],\n",
       "       [1, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 4, 2, 0, 0, 0, 0],\n",
       "       [6, 6, 3, 2, 6, 1, 6],\n",
       "       [1, 6, 3, 6, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras pad_sequences 모듈 쓰면 편한\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(encoded_sentences,padding='post') # padding='pre' 앞에 생김, default\n",
    "padded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e36431c",
   "metadata": {},
   "source": [
    "## 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "54f519e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "tokens = okt.morphs('나는 자연어 처리를 배운다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c3fc775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word : index for index, word in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7080ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word_to_index):\n",
    "    one_hot_vector = [0] * (len(word_to_index))\n",
    "    index = word_to_index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "25feaa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\", word_to_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee9de071",
   "metadata": {},
   "source": [
    "# 2.케라스(Keras)를 이용한 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "eac44cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text]) # 1개 리스트 안에 넣어줘야함 ,string으로 넣으면 이상하게 나옴\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e575e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e094fadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 5, 1, 2, 6, 3, 1, 1, 3, 7]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9082fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = to_categorical(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5aef4bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot\n",
    "# 원핫인코딩은 문장에 단어가 1000개면 리스트가 1000개 생겨야함 ㅋ 너무 성능 안조음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0d530243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # 머신러닝 트레인, 테스트 데이터셋 나누기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffe843c2",
   "metadata": {},
   "source": [
    "## 1. 지도 학습(Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "354d6e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, (4, 4))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(0,16).reshape(4,4)\n",
    "arr.size, arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 4,  5,  6],\n",
       "       [ 8,  9, 10],\n",
       "       [12, 13, 14]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:,:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cbadb7b",
   "metadata": {},
   "source": [
    "# 한국어 전처리 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "07980bdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soynlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msoynlp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'soynlp'"
     ]
    }
   ],
   "source": [
    "import soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3880813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
